{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMACU1kWOkQmB2pe3SIHtmy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaidenFlint/Project3_Group2/blob/Kavita/Project3drugsQ%26A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukogl9s-OsQP",
        "outputId": "6aa01fdc-c9bf-4c0a-a412-8d1e401e3ba1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "\n",
        "# Web Scraping Function\n",
        "def scrape_drug_info(drug_name):\n",
        "    url = f'https://www.drugs.com/{drug_name}.html'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    drug_info = {}\n",
        "\n",
        "    # Find description\n",
        "    description_div = soup.find(\"div\", class_=\"content\")\n",
        "    if description_div:\n",
        "        description_paragraphs = description_div.find_all(\"p\")\n",
        "        drug_info['description'] = \" \".join([p.text.strip() for p in description_paragraphs])\n",
        "    else:\n",
        "        drug_info['description'] = \"Description not found.\"\n",
        "\n",
        "    # Find uses\n",
        "    uses_section = soup.find(string=\"Uses\")\n",
        "    if uses_section:\n",
        "        drug_info['uses'] = uses_section.find_next(\"p\").text.strip()\n",
        "    else:\n",
        "        drug_info['uses'] = \"Uses not found.\"\n",
        "\n",
        "    # Find side effects\n",
        "    side_effects_section = soup.find(string=\"Side effects\")\n",
        "    if side_effects_section:\n",
        "        drug_info['side_effects'] = side_effects_section.find_next(\"p\").text.strip()\n",
        "    else:\n",
        "        drug_info['side_effects'] = \"Side effects not found.\"\n",
        "\n",
        "    return drug_info\n",
        "\n",
        "# Scrape multiple drugs\n",
        "drugs = ['aspirin', 'ibuprofen', 'metformin']\n",
        "drug_data = []\n",
        "\n",
        "for drug in drugs:\n",
        "    info = scrape_drug_info(drug)\n",
        "    if info:\n",
        "        drug_data.append(info)\n",
        "\n",
        "# Convert to DataFrame\n",
        "drug_df = pd.DataFrame(drug_data)\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "drug_df.to_csv('drug_info.csv', index=False)"
      ],
      "metadata": {
        "id": "r7gVWo--ei3f"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for multi-label classification\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Simulated drug_df for demonstration purposes\n",
        "data = {'description': ['pain relief', 'nausea treatment', 'pain and fever relief'],\n",
        "        'uses': ['pain', 'nausea', 'pain']}\n",
        "drug_df = pd.DataFrame(data)\n",
        "\n",
        "# Prepare data for model training\n",
        "X = drug_df['description']  # Features\n",
        "y = drug_df['uses'].copy()  # Focus on one target variable for simplicity\n",
        "\n",
        "# Encode labels (for simplicity, using string matching)\n",
        "y = y.apply(lambda x: 1 if 'pain' in x.lower() else 0)  # Dummy encoding\n",
        "\n",
        "# Check the unique values in the labels to identify any unexpected values\n",
        "print(\"Unique values in 'uses':\", y.unique())\n",
        "print(\"Class distribution in 'uses':\")\n",
        "print(y.value_counts())  # Check the class distribution\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline for vectorization and model training\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('classifier', MultinomialNB(class_prior=[0.33, 0.67]))  # Adjust class priors based on class distribution\n",
        "])\n",
        "\n",
        "# Initialize Leave-One-Out Cross-Validation\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "# Fit the model\n",
        "try:\n",
        "    pipeline.fit(X_train, y_train)\n",
        "except ValueError as e:\n",
        "    print(\"Error while fitting the model:\", e)\n",
        "\n",
        "# Validate the model\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the trained model\n",
        "# joblib.dump(pipeline, 'best_drug_model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KfO3ussits-",
        "outputId": "6f45d5fe-2cda-48ef-e475-f85362e6dde1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in 'uses': [1 0]\n",
            "Class distribution in 'uses':\n",
            "uses\n",
            "1    2\n",
            "0    1\n",
            "Name: count, dtype: int64\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       1.00      1.00      1.00         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning using Grid Search\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Simulated drug_df for demonstration purposes\n",
        "data = {'description': ['pain relief', 'nausea treatment', 'pain and fever relief'],\n",
        "        'uses': ['pain', 'nausea', 'pain']}\n",
        "drug_df = pd.DataFrame(data)\n",
        "\n",
        "# Prepare data for model training\n",
        "X = drug_df['description']  # Features\n",
        "y = drug_df['uses'].copy()  # Target variable\n",
        "\n",
        "# Encode labels (for simplicity, using string matching)\n",
        "y = y.apply(lambda x: 1 if 'pain' in x.lower() else 0)  # Dummy encoding\n",
        "\n",
        "# Check the unique values in the labels to identify any unexpected values\n",
        "print(\"Unique values in 'uses':\", y.unique())\n",
        "print(\"Class distribution in 'uses':\")\n",
        "print(y.value_counts())  # Check the class distribution\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline for vectorization and model training\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('classifier', MultinomialNB())  # Use only one classifier\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning using Grid Search\n",
        "param_grid = {\n",
        "    'classifier__alpha': [0.1, 0.5, 1.0, 1.5, 2.0]  # Hyperparameter to tune\n",
        "}\n",
        "\n",
        "# Initialize Leave-One-Out Cross-Validation\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "# Initialize GridSearchCV with Leave-One-Out\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=loo, n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "try:\n",
        "    grid_search.fit(X_train, y_train)\n",
        "except ValueError as e:\n",
        "    print(\"Error while fitting the model:\", e)\n",
        "\n",
        "# Check if the model fitted successfully\n",
        "if hasattr(grid_search, 'best_estimator_'):\n",
        "    # Best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Validate the model\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "else:\n",
        "    print(\"Model fitting failed; unable to retrieve best estimator.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xiv8_sAclNNE",
        "outputId": "faafd169-076c-436d-8a6c-d623a1bbe92d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in 'uses': [1 0]\n",
            "Class distribution in 'uses':\n",
            "uses\n",
            "1    2\n",
            "0    1\n",
            "Name: count, dtype: int64\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       1.00      1.00      1.00         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build text classification model using Support Vector Machines (SVM)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Simulated drug_df for demonstration purposes\n",
        "data = {'description': ['pain relief', 'nausea treatment', 'pain and fever relief'],\n",
        "        'uses': ['pain', 'nausea', 'pain']}\n",
        "drug_df = pd.DataFrame(data)\n",
        "\n",
        "# Prepare data for model training\n",
        "X = drug_df['description']  # Features\n",
        "y = drug_df['uses'].copy()  # Target variable\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorization using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize Support Vector Machine model\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Print classification report, specifying labels\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_, labels=range(len(label_encoder.classes_))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d0l2fcPo5Pn",
        "outputId": "669c9014-ff44-42a1-e52b-1aaa091194b1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      nausea       0.00      0.00      0.00         0\n",
            "        pain       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       0.50      0.50      0.50         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text classification model using a Convolutional Neural Network (CNN) with TensorFlow/Keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Simulated drug_df for demonstration purposes\n",
        "data = {'description': ['pain relief', 'nausea treatment', 'pain and fever relief'],\n",
        "        'uses': ['pain', 'nausea', 'pain']}\n",
        "drug_df = pd.DataFrame(data)\n",
        "\n",
        "# Prepare data for model training\n",
        "X = drug_df['description']  # Features\n",
        "y = drug_df['uses'].copy()  # Target variable\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Tokenization\n",
        "max_words = 1000  # Maximum number of words to consider\n",
        "max_len = 20      # Maximum length of each input sequence\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_encoded = tokenizer.texts_to_sequences(X)\n",
        "X_padded = pad_sequences(X_encoded, maxlen=max_len)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128))  # Removed input_length argument\n",
        "model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.5))  # Added dropout layer\n",
        "model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.01)))  # L2 regularization\n",
        "model.add(Dropout(0.5))  # Added dropout layer\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "try:\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=2, validation_split=0.2)\n",
        "except Exception as e:\n",
        "    print(\"Error while fitting the model:\", e)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Predict on new data\n",
        "predictions = model.predict(X_test)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "print(\"Predicted classes:\", label_encoder.inverse_transform(predicted_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvcS8h7Anhuz",
        "outputId": "11da9aea-8f54-4425-a165-0fd1c52e44d1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 1.0000 - loss: 0.8694 - val_accuracy: 0.0000e+00 - val_loss: 0.9689\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 1.0000 - loss: 0.7974 - val_accuracy: 0.0000e+00 - val_loss: 0.9725\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.0000e+00 - loss: 0.9327 - val_accuracy: 0.0000e+00 - val_loss: 0.9769\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.8408 - val_accuracy: 0.0000e+00 - val_loss: 0.9804\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.7768 - val_accuracy: 0.0000e+00 - val_loss: 0.9836\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.7670 - val_accuracy: 0.0000e+00 - val_loss: 0.9865\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.0000e+00 - loss: 0.9090 - val_accuracy: 0.0000e+00 - val_loss: 0.9894\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.7632 - val_accuracy: 0.0000e+00 - val_loss: 0.9929\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 1.0000 - loss: 0.8759 - val_accuracy: 0.0000e+00 - val_loss: 0.9968\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.8535 - val_accuracy: 0.0000e+00 - val_loss: 0.9990\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0000e+00 - loss: 0.9410\n",
            "Test Loss: 0.9410, Test Accuracy: 0.0000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "Predicted classes: ['nausea']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using SVM\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Expanded dataset for demonstration purposes\n",
        "data = {\n",
        "    'description': [\n",
        "        'pain relief', 'nausea treatment', 'pain and fever relief',\n",
        "        'nausea', 'headache relief', 'pain medication',\n",
        "        'anti-nausea medication', 'nausea and vomiting',\n",
        "        'pain management strategies', 'over-the-counter pain relief',\n",
        "        'pain and nausea', 'migraines treatment', 'chronic pain relief',\n",
        "        'post-operative nausea', 'acute nausea management'\n",
        "    ],\n",
        "    'uses': [\n",
        "        'pain', 'nausea', 'pain', 'nausea', 'pain',\n",
        "        'pain', 'nausea', 'nausea', 'pain', 'pain',\n",
        "        'pain', 'nausea', 'pain', 'nausea', 'nausea'\n",
        "    ]\n",
        "}\n",
        "drug_df = pd.DataFrame(data)\n",
        "\n",
        "# Prepare data for model training\n",
        "X = drug_df['description']  # Features\n",
        "y = drug_df['uses'].copy()  # Target variable\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data into train and test sets with stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Vectorization using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize Support Vector Machine model\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Print classification report, specifying zero_division parameter\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VZMDN7npc60",
        "outputId": "674fb72c-57a1-42ba-f030-93b4a5d11edc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      nausea       1.00      1.00      1.00         1\n",
            "        pain       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         3\n",
            "   macro avg       1.00      1.00      1.00         3\n",
            "weighted avg       1.00      1.00      1.00         3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Initialize Support Vector Machine model\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(model, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Mean cross-validation score:\", np.mean(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBqE3q58rDSX",
        "outputId": "3138937d-93db-4c82-88c5-be9c53364bef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [1. 1. 1. 1. 1.]\n",
            "Mean cross-validation score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model using a simple neural network with TensorFlow/Keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "# Expanded dataset for demonstration purposes\n",
        "data = {\n",
        "    'description': [\n",
        "        'pain relief', 'nausea treatment', 'pain and fever relief',\n",
        "        'nausea', 'headache relief', 'pain medication',\n",
        "        'anti-nausea medication', 'nausea and vomiting',\n",
        "        'pain management strategies', 'over-the-counter pain relief',\n",
        "        'pain and nausea', 'migraines treatment', 'chronic pain relief',\n",
        "        'post-operative nausea', 'acute nausea management'\n",
        "    ],\n",
        "    'uses': [\n",
        "        'pain', 'nausea', 'pain', 'nausea', 'pain',\n",
        "        'pain', 'nausea', 'nausea', 'pain', 'pain',\n",
        "        'pain', 'nausea', 'pain', 'nausea', 'nausea'\n",
        "    ]\n",
        "}\n",
        "drug_df = pd.DataFrame(data)\n",
        "\n",
        "# Prepare data for model training\n",
        "X = drug_df['description']  # Features\n",
        "y = drug_df['uses'].copy()  # Target variable\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "# Vectorization using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Create a simple neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', input_shape=(X_train_tfidf.shape[1],)))\n",
        "model.add(Dense(2, activation='softmax'))  # 2 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up TensorBoard callback\n",
        "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_tfidf.toarray(), y_train, epochs=10, batch_size=5, validation_data=(X_test_tfidf.toarray(), y_test), callbacks=[tensorboard_callback])\n",
        "\n",
        "# After training, you can run TensorBoard with the following command in the terminal:\n",
        "# tensorboard --logdir=./logs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MlFggzWSGV5",
        "outputId": "0d91ee90-4036-40e9-9a72-f87fb6c3ce7e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 192ms/step - accuracy: 0.6417 - loss: 0.6653 - val_accuracy: 0.6667 - val_loss: 0.6427\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6833 - loss: 0.6634 - val_accuracy: 0.6667 - val_loss: 0.6371\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.7083 - loss: 0.6495 - val_accuracy: 0.6667 - val_loss: 0.6314\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.7000 - loss: 0.6772 - val_accuracy: 0.6667 - val_loss: 0.6261\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7500 - loss: 0.6579 - val_accuracy: 0.6667 - val_loss: 0.6208\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7500 - loss: 0.6501 - val_accuracy: 0.6667 - val_loss: 0.6157\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.7000 - loss: 0.6508 - val_accuracy: 0.6667 - val_loss: 0.6106\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228ms/step - accuracy: 0.8250 - loss: 0.6192 - val_accuracy: 0.6667 - val_loss: 0.6058\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.7750 - loss: 0.6354 - val_accuracy: 0.6667 - val_loss: 0.6011\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.7750 - loss: 0.6184 - val_accuracy: 0.6667 - val_loss: 0.5964\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7839c7488f10>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Web Scraping and User Interaction\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_drug_info(drug_name):\n",
        "    url = f'https://www.drugs.com/{drug_name}.html'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve data for {drug_name}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    drug_info = {}\n",
        "    drug_info['name'] = drug_name.capitalize()\n",
        "\n",
        "    # Find the description\n",
        "    description_div = soup.find(\"div\", class_=\"content\")  # Adjust the selector as needed\n",
        "    if description_div:\n",
        "        description_paragraphs = description_div.find_all(\"p\")\n",
        "        drug_info['description'] = \" \".join([p.text.strip() for p in description_paragraphs])\n",
        "    else:\n",
        "        drug_info['description'] = \"Description not found.\"\n",
        "\n",
        "    # Find uses\n",
        "    uses_section = soup.find(string=\"Uses\")\n",
        "    if uses_section:\n",
        "        drug_info['uses'] = uses_section.find_next(\"p\").text.strip()\n",
        "    else:\n",
        "        drug_info['uses'] = \"Uses not found.\"\n",
        "\n",
        "    # Find side effects\n",
        "    side_effects_section = soup.find(string=\"Side effects\")\n",
        "    if side_effects_section:\n",
        "        drug_info['side_effects'] = side_effects_section.find_next(\"p\").text.strip()\n",
        "    else:\n",
        "        drug_info['side_effects'] = \"Side effects not found.\"\n",
        "\n",
        "    return drug_info\n",
        "\n",
        "def main():\n",
        "    while True:\n",
        "        drug_name = input(\"Enter the drug name (or type 'exit' to quit): \").strip().lower()\n",
        "        if drug_name == 'exit':\n",
        "            print(\"Exiting the program.\")\n",
        "            break\n",
        "\n",
        "        info = scrape_drug_info(drug_name)\n",
        "        if info:\n",
        "            print(f\"\\nDrug Name: {info['name']}\")\n",
        "            print(f\"Uses: {info['uses']}\")\n",
        "            print(f\"Side Effects: {info['side_effects']}\\n\")\n",
        "        else:\n",
        "            print(\"No information found.\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02Yh7HvuQN9x",
        "outputId": "4fe0dfbb-64f7-4800-89d1-33a69f358cf2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the drug name (or type 'exit' to quit): apixaban\n",
            "Failed to retrieve data for apixaban. Status code: 404\n",
            "No information found.\n",
            "\n",
            "Enter the drug name (or type 'exit' to quit): addrall\n",
            "Failed to retrieve data for addrall. Status code: 404\n",
            "No information found.\n",
            "\n",
            "Enter the drug name (or type 'exit' to quit): aspirin\n",
            "\n",
            "Drug Name: Aspirin\n",
            "Uses: Aspirin is a salicylate (sa-LIS-il-ate). It works by reducing substances in the body that cause pain, fever, and inflammation.\n",
            "Side Effects: Aspirin is a salicylate (sa-LIS-il-ate). It works by reducing substances in the body that cause pain, fever, and inflammation.\n",
            "\n",
            "Enter the drug name (or type 'exit' to quit): exit\n",
            "Exiting the program.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install gradio transformers langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osjRP3pJOsC-",
        "outputId": "e5996413-b4d5-484a-bf9f-be98a0ee1da2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.6.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.5)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.4.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.3)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.0)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.3->gradio) (2024.9.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.3->gradio) (12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.19)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Integration with LangChain and Gradio\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load pre-trained model and tokenizer from Hugging Face\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Define labels, indications, and side effects for various medications\n",
        "labels_info = {\n",
        "    'pain': {\n",
        "        'indications': \"Used for alleviating various types of pain.\",\n",
        "        'side_effects': \"Possible side effects include dizziness, nausea, and constipation.\"\n",
        "    },\n",
        "    'nausea': {\n",
        "        'indications': \"Used for treating nausea and vomiting.\",\n",
        "        'side_effects': \"Possible side effects include drowsiness, dry mouth, and fatigue.\"\n",
        "    },\n",
        "    'anticoagulant': {\n",
        "        'indications': \"Used to prevent blood clots.\",\n",
        "        'side_effects': \"Possible side effects include bleeding, bruising, and gastrointestinal issues.\"\n",
        "    },\n",
        "    'antiplatelet': {\n",
        "        'indications': \"Used to prevent blood clots by inhibiting platelet aggregation.\",\n",
        "        'side_effects': \"Possible side effects include bleeding, gastrointestinal disturbances, and rash.\"\n",
        "    },\n",
        "    'benzodiazepine': {\n",
        "        'indications': \"Used to treat anxiety disorders and panic disorders.\",\n",
        "        'side_effects': \"Possible side effects include drowsiness, dizziness, and fatigue.\"\n",
        "    },\n",
        "    'opioid': {\n",
        "        'indications': \"Used for managing severe pain.\",\n",
        "        'side_effects': \"Possible side effects include drowsiness, constipation, and nausea.\"\n",
        "    },\n",
        "    'antiemetic': {\n",
        "        'indications': \"Used to prevent nausea and vomiting.\",\n",
        "        'side_effects': \"Possible side effects include headache, dizziness, and constipation.\"\n",
        "    },\n",
        "    'calcium_channel_blocker': {\n",
        "        'indications': \"Used to treat high blood pressure and angina.\",\n",
        "        'side_effects': \"Possible side effects include dizziness, flushing, and headache.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to classify input text and provide indications and side effects\n",
        "def classify_text(text):\n",
        "    # Simulate a more controlled output based on known medications\n",
        "    text_lower = text.lower()\n",
        "    if \"acetaminophen\" in text_lower:\n",
        "        label = 'pain'\n",
        "    elif \"apixaban\" in text_lower:\n",
        "        label = 'anticoagulant'\n",
        "    elif \"clopidogrel\" in text_lower:\n",
        "        label = 'antiplatelet'\n",
        "    elif \"alprazolam\" in text_lower:\n",
        "        label = 'benzodiazepine'\n",
        "    elif \"norco\" in text_lower:\n",
        "        label = 'opioid'\n",
        "    elif \"zofran\" in text_lower:\n",
        "        label = 'antiemetic'\n",
        "    elif \"diltiazem\" in text_lower:\n",
        "        label = 'calcium_channel_blocker'\n",
        "    elif \"nausea\" in text_lower:\n",
        "        label = 'nausea'\n",
        "    else:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class = logits.argmax(dim=-1).item()  # Get the index of the highest score\n",
        "        label = list(labels_info.keys())[predicted_class]\n",
        "\n",
        "    # Prepare output information\n",
        "    indications = labels_info[label]['indications']\n",
        "    side_effects = labels_info[label]['side_effects']\n",
        "\n",
        "    return f\"Classification: {label}\\nIndications: {indications}\\nSide Effects: {side_effects}\"\n",
        "\n",
        "# Set up Gradio interface\n",
        "iface = gr.Interface(fn=classify_text,\n",
        "                     inputs=\"text\",\n",
        "                     outputs=\"text\",\n",
        "                     title=\"Text Classification with Indications and Side Effects\",\n",
        "                     description=\"Input a text description, and the model will classify it, providing indications and side effects.\")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "ZzCmLM5wOsAI",
        "outputId": "c6d5f182-0e23-48d7-b21d-f5a78247934b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3868e1177251021e77.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3868e1177251021e77.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}